import { NextRequest, NextResponse } from "next/server";
import { ChatOpenAI } from "@langchain/openai";
import { ChatOllama } from "@langchain/ollama";
import { ChatPromptTemplate } from "@langchain/core/prompts";
import { StringOutputParser } from "@langchain/core/output_parsers";
import { RunnablePassthrough } from "@langchain/core/runnables";

// å®šä¹‰ LLM é…ç½®æ¥å£
interface LLMConfig {
  modelName: string;
  apiKey: string;
  baseUrl?: string;
  llmType: "openai" | "ollama";
  temperature?: number;
  maxTokens?: number;
  maxRetries?: number;
  topP?: number;
  frequencyPenalty?: number;
  presencePenalty?: number;
  topK?: number;
  repeatPenalty?: number;
  streaming?: boolean;
  streamUsage?: boolean;
  language?: "zh" | "en";
}

// åˆ›å»º LLM å®ä¾‹
function createLLM(config: LLMConfig): ChatOpenAI | ChatOllama {
  const safeModel = config.modelName?.trim() || "";
  const defaultSettings = {
    temperature: 0.7,
    maxTokens: undefined,
    timeout: 1000000000,
    maxRetries: 0,
    topP: 0.7,
    frequencyPenalty: 0,
    presencePenalty: 0,
    topK: 40,
    repeatPenalty: 1.1,
    streaming: false,
    streamUsage: true, // é»˜è®¤å¯ç”¨token usageè¿½è¸ª
  };

  if (config.llmType === "openai") {
    return new ChatOpenAI({
      modelName: safeModel,
      openAIApiKey: config.apiKey,
      configuration: {
        baseURL: config.baseUrl?.trim() || undefined,
      },
      temperature: config.temperature ?? defaultSettings.temperature,
      maxRetries: config.maxRetries ?? defaultSettings.maxRetries,
      topP: config.topP ?? defaultSettings.topP,
      frequencyPenalty: config.frequencyPenalty ?? defaultSettings.frequencyPenalty,
      presencePenalty: config.presencePenalty ?? defaultSettings.presencePenalty,
      streaming: config.streaming ?? defaultSettings.streaming,
      streamUsage: config.streamUsage ?? defaultSettings.streamUsage,
    });
  } else if (config.llmType === "ollama") {
    return new ChatOllama({
      model: safeModel,
      baseUrl: config.baseUrl?.trim() || "http://localhost:11434",
      temperature: config.temperature ?? defaultSettings.temperature,
      topK: config.topK ?? defaultSettings.topK,
      topP: config.topP ?? defaultSettings.topP,
      frequencyPenalty: config.frequencyPenalty ?? defaultSettings.frequencyPenalty,
      presencePenalty: config.presencePenalty ?? defaultSettings.presencePenalty,
      repeatPenalty: config.repeatPenalty ?? defaultSettings.repeatPenalty,
      streaming: config.streaming ?? defaultSettings.streaming,
    });
  } else {
    throw new Error(`Unsupported LLM type: ${config.llmType}`);
  }
}

// åˆ›å»ºå¯¹è¯é“¾
function createDialogueChain(llm: ChatOpenAI | ChatOllama): any {
  const dialoguePrompt = ChatPromptTemplate.fromMessages([
    ["system", "{system_message}"],
    ["human", "{user_message}"],
  ]);

  return RunnablePassthrough.assign({
    system_message: (input: any) => input.system_message,
    user_message: (input: any) => input.user_message,
  })
    .pipe(dialoguePrompt)
    .pipe(llm)
    .pipe(new StringOutputParser());
}

// ä¸»è¦çš„ LLM è°ƒç”¨å‡½æ•°
async function invokeLLM(
  systemMessage: string,
  userMessage: string,
  config: LLMConfig,
): Promise<{ response: string; tokenUsage?: any }> {
  try {
    console.log("invokeLLM");

    // ä¸ºäº†è·å–çœŸå®çš„token usageï¼Œæˆ‘ä»¬éœ€è¦ç›´æ¥è°ƒç”¨LLMè€Œä¸æ˜¯ä½¿ç”¨chain
    if (config.llmType === "openai") {
      const openaiLlm = createLLM(config) as ChatOpenAI;

      // ç›´æ¥è°ƒç”¨LLMè·å–å®Œæ•´çš„AIMessageå“åº”
      const aiMessage = await openaiLlm.invoke([
        { role: "system", content: systemMessage },
        { role: "user", content: userMessage },
      ]);

      // æå–token usageä¿¡æ¯
      let tokenUsage = null;
      if (aiMessage.usage_metadata) {
        tokenUsage = {
          prompt_tokens: aiMessage.usage_metadata.input_tokens,
          completion_tokens: aiMessage.usage_metadata.output_tokens,
          total_tokens: aiMessage.usage_metadata.total_tokens,
        };
      } else if (aiMessage.response_metadata?.tokenUsage) {
        // å…¼å®¹æ—§ç‰ˆæœ¬æ ¼å¼
        tokenUsage = aiMessage.response_metadata.tokenUsage;
      } else if (aiMessage.response_metadata?.usage) {
        // å…¼å®¹å¦ä¸€ç§æ ¼å¼
        tokenUsage = aiMessage.response_metadata.usage;
      }

      // å¦‚æœæ²¡æœ‰ä»å“åº”ä¸­è·å–åˆ°token usageï¼Œå°è¯•ä»æµå¼å“åº”ä¸­è·å–
      if (!tokenUsage && config.streaming && config.streamUsage) {
        console.log("ğŸ“Š Token usage not found in response, this may be due to streaming mode");
      }

      return {
        response: aiMessage.content as string,
        tokenUsage,
      };
    } else {
      // å¯¹äºå…¶ä»–LLMç±»å‹ï¼Œä½¿ç”¨åŸæ¥çš„chainæ–¹å¼
      const llm = createLLM(config);
      const dialogueChain = createDialogueChain(llm);
      const response = await dialogueChain.invoke({
        system_message: systemMessage,
        user_message: userMessage,
      });

      if (!response || typeof response !== "string") {
        throw new Error("Invalid response from LLM");
      }

      return { response };
    }
  } catch (error) {
    console.error("Error in invokeLLM:", error);
    throw error;
  }
}

// POST è¯·æ±‚å¤„ç†å‡½æ•°
export async function POST(request: NextRequest) {
  try {
    const body = await request.json();
    const { systemMessage, userMessage, config } = body;

    // éªŒè¯å¿…è¦å‚æ•°
    if (!systemMessage || !userMessage || !config) {
      return NextResponse.json(
        { success: false, error: "Missing required parameters" },
        { status: 400 },
      );
    }

    // è°ƒç”¨ LLM
    const result = await invokeLLM(systemMessage, userMessage, config);

    // è¿”å›ç»“æœ
    return NextResponse.json({
      success: true,
      response: result.response,
      tokenUsage: result.tokenUsage,
    });
  } catch (error: any) {
    console.error("LLM API error:", error);
    return NextResponse.json(
      { success: false, error: error.message || "Failed to process LLM request" },
      { status: 500 },
    );
  }
}
